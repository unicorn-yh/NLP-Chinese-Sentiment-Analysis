{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ls = []\n",
    "def getStopWord():\n",
    "    with open('lib/stopwords_utf8.txt', 'r',encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            stopword_ls.append(line.split('\\n')[0])\n",
    "getStopWord()\n",
    "\n",
    "def isStopWord(word):\n",
    "    for i in range(len(stopword_ls)):\n",
    "        if word == stopword_ls[i]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def load_data():   #获取数据集\n",
    "    data = []\n",
    "    getStopWord()\n",
    "    positive_line = 0\n",
    "    negative_line = 0\n",
    "    total_positive_line = 7000\n",
    "    total_negative_line = 3000\n",
    "\n",
    "    with open('data/total/pos.txt', 'r', encoding='utf-8') as f:\n",
    "        print('File Directory: data/total/pos.txt')\n",
    "        sentences = f.readlines()\n",
    "        for sentence in sentences[:total_positive_line]:\n",
    "            positive_line += 1\n",
    "            if positive_line == total_positive_line:\n",
    "                end_val = '\\n'\n",
    "            else:\n",
    "                end_val = '\\r'\n",
    "            print('Getting positive sentence {}/{}'.format(positive_line,total_positive_line),end=end_val)\n",
    "            word_ls = []\n",
    "            words = sentence.replace('\\n','').split('    ')   #get chinese sentence\n",
    "            tmp_ls = list(jieba.cut(words[1], cut_all=True))   #segmentation\n",
    "            for i in range(len(tmp_ls)):\n",
    "                if not isStopWord(tmp_ls[i]):\n",
    "                       word_ls.append(tmp_ls[i]) \n",
    "            data.append([word_ls, 1])\n",
    "\n",
    "    with open('data/total/neg.txt', 'r', encoding='utf-8') as f:\n",
    "        print('File Directory: data/total/neg.txt')\n",
    "        sentences = f.readlines()\n",
    "        for sentence in sentences[:total_negative_line]:\n",
    "            negative_line += 1\n",
    "            if negative_line == total_negative_line:\n",
    "                end_val = '\\n'\n",
    "            else:\n",
    "                end_val = '\\r'\n",
    "            print('Getting negative sentence {}/{}'.format(negative_line,total_negative_line),end=end_val)\n",
    "            word_ls = []\n",
    "            words = sentence.replace('\\n','').split('    ')   #get chinese sentence\n",
    "            tmp_ls = list(jieba.cut(words[1], cut_all=True))   #segmentation\n",
    "            for i in range(len(tmp_ls)):\n",
    "                if not isStopWord(tmp_ls[i]):\n",
    "                       word_ls.append(tmp_ls[i]) \n",
    "            data.append([word_ls, 0])\n",
    "\n",
    "    print('Positive Line: {} | Negative Line: {}'.format(positive_line,negative_line))\n",
    "    random.shuffle(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Directory: data/total/pos.txt\n",
      "Getting positive sentence 1/7000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.762 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting positive sentence 7000/7000\n",
      "File Directory: data/total/neg.txt\n",
      "Getting negative sentence 3000/3000\n",
      "Positive Line: 7000 | Negative Line: 3000\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = train_test_split(load_data(), test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET | Total Words:397486 | Total Vocabulary:27639 | Max Sentence Length:1005\n",
      "TEST  DATASET | Total Words:42659  | Total Vocabulary:9486 | Max Sentence Length:366\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens,_ in data_train for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens,_ in data_train]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"TRAIN DATASET | Total Words:{} | Total Vocabulary:{} | Max Sentence Length:{}\".format(len(all_training_words),len(TRAINING_VOCAB),max(training_sentence_lengths)))\n",
    "\n",
    "all_test_words = [word for tokens,_ in data_test for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens,_ in data_test]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"TEST  DATASET | Total Words:{}  | Total Vocabulary:{} | Max Sentence Length:{}\" .format(len(all_test_words),len(TEST_VOCAB),max(test_sentence_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_dict(cleaned_tokens):\n",
    "    return dict([token, True] for token in cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'字': True,\n",
       "  '差': True,\n",
       "  '入住': True,\n",
       "  '时': True,\n",
       "  '开': True,\n",
       "  '房卡': True,\n",
       "  '刷': True,\n",
       "  '不开': True,\n",
       "  '开门': True,\n",
       "  '管理': True,\n",
       "  '从前': True,\n",
       "  '前台': True,\n",
       "  '房间': True,\n",
       "  '换乘': True,\n",
       "  '电梯': True,\n",
       "  '地铁': True,\n",
       "  '硬件': True,\n",
       "  '没中': True,\n",
       "  '中央': True,\n",
       "  '中央空调': True,\n",
       "  '空调': True,\n",
       "  '4XX': True,\n",
       "  '元': True,\n",
       "  '性价比': True,\n",
       "  '一分': True},\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train = [(list_to_dict(tokens),label) for tokens,label in data_train ]\n",
    "final_test = [(list_to_dict(tokens),label) for tokens,label in data_test ]\n",
    "final_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start_time = time()\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(final_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data: 0.7836\n",
      "Accuracy on test  data: 0.7240\n",
      "Most Informative Features\n",
      "                      恶劣 = True                0 : 1      =     60.0 : 1.0\n",
      "                    再也不会 = True                0 : 1      =     38.6 : 1.0\n",
      "                      黑店 = True                0 : 1      =     38.0 : 1.0\n",
      "                      最差 = True                0 : 1      =     29.4 : 1.0\n",
      "                      欺骗 = True                0 : 1      =     29.3 : 1.0\n",
      "                    强烈要求 = True                0 : 1      =     27.1 : 1.0\n",
      "                       烂 = True                0 : 1      =     26.0 : 1.0\n",
      "                      不堪 = True                0 : 1      =     25.6 : 1.0\n",
      "                      最糟 = True                0 : 1      =     25.6 : 1.0\n",
      "                      责任 = True                0 : 1      =     24.0 : 1.0\n",
      "                      质问 = True                0 : 1      =     24.0 : 1.0\n",
      "                      高价 = True                0 : 1      =     24.0 : 1.0\n",
      "                      可气 = True                0 : 1      =     22.8 : 1.0\n",
      "                      一流 = True                1 : 0      =     22.8 : 1.0\n",
      "                      极差 = True                0 : 1      =     22.7 : 1.0\n",
      "                   ！！！！！ = True                0 : 1      =     22.5 : 1.0\n",
      "                     洗不掉 = True                0 : 1      =     20.9 : 1.0\n",
      "                    大失所望 = True                0 : 1      =     19.4 : 1.0\n",
      "                      失所 = True                0 : 1      =     19.4 : 1.0\n",
      "                      当差 = True                0 : 1      =     19.4 : 1.0\n",
      "None\n",
      "CPU Time: 6.386597633361816\n"
     ]
    }
   ],
   "source": [
    "# Output the model accuracy on the train and test data\n",
    "print('Accuracy on train data: {:.4f}'.format(classify.accuracy(classifier, final_train)))\n",
    "print('Accuracy on test  data: {:.4f}'.format(classify.accuracy(classifier, final_test)))\n",
    "\n",
    "# Output Top 20 Sentiment Word\n",
    "print(classifier.show_most_informative_features(20))\n",
    "print('CPU Time:', time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_segment(text):\n",
    "    word_ls = []\n",
    "    tmp_ls = list(jieba.cut(text, cut_all=False))   #segmentation\n",
    "    for i in range(len(tmp_ls)):\n",
    "        if not isStopWord(tmp_ls[i]):\n",
    "            word_ls.append(tmp_ls[i]) \n",
    "    return word_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Naive Bayes Test Data----------\n",
      "Getting positive data: 1000/1000\n",
      "Positive Count: 278 | Negative Count: 722\n",
      "Getting negative data: 1000/1000\n",
      "Positive Count: 971 | Negative Count: 29\n",
      "Accuracy: 0.1535\n"
     ]
    }
   ],
   "source": [
    "#TEST DATASET\n",
    "print('---------Naive Bayes Test Data----------')\n",
    "dataset_size = 1000\n",
    "correct_count = 0\n",
    "type = ['pos','neg']\n",
    "for j in range(len(type)):\n",
    "      positive_count,negative_count = 0,0\n",
    "      if type[j] == 'pos':\n",
    "            st = 'positive'\n",
    "      else:\n",
    "            st = 'negative'\n",
    "      for i in range(dataset_size):\n",
    "            with open('data/'+st+'/'+type[j]+'.'+str(i)+'.txt','r',encoding='UTF-8') as file:\n",
    "                  text = file.read().replace('\\n', '')\n",
    "                  if i == dataset_size-1:\n",
    "                        end_val = '\\n'\n",
    "                  else:\n",
    "                        end_val = '\\r'\n",
    "                  \n",
    "                  print('Getting '+st+' data: {}/{}'.format(i+1,dataset_size),end=end_val)\n",
    "\n",
    "            custom_tokens = jieba_segment(text)\n",
    "\n",
    "            sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "            if sentiment == 0:\n",
    "                  #print(\"Positive Sentiment\")\n",
    "                  positive_count += 1\n",
    "            else:\n",
    "                  #print(\"Negative Sentiment\")\n",
    "                  negative_count += 1\n",
    "\n",
    "      print('Positive Count:',positive_count,end = ' | ')\n",
    "      print('Negative Count:',negative_count)\n",
    "      if type[j] == 'pos':\n",
    "            correct_count += positive_count\n",
    "      else:\n",
    "            correct_count += negative_count\n",
    "print('Accuracy: {}'.format(correct_count/(2*dataset_size)))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment 负面情绪\n"
     ]
    }
   ],
   "source": [
    "text = \"房间设施难以够得上五星级，服务还不错，有送水果。\"\n",
    "custom_tokens = jieba_segment(text)\n",
    "sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "if sentiment == 0:\n",
    "      print(\"Positive Sentiment 正面情绪\")\n",
    "else:\n",
    "      print(\"Negative Sentiment 负面情绪\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment 正面情绪\n"
     ]
    }
   ],
   "source": [
    "text = \"前台服务较差，不为客户着想。房间有朋友来需要打扫，呼叫了两个小时也未打扫。房间下水道臭气熏天，卫生间漏水堵水。\"\n",
    "custom_tokens = jieba_segment(text)\n",
    "sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "if sentiment == 0:\n",
    "      print(\"Positive Sentiment 正面情绪\")\n",
    "else:\n",
    "      print(\"Negative Sentiment 负面情绪\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db9c657620c8578ddfc3d9ba89c38fdc5fd80f960e156f76bd6d1a512183127c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
