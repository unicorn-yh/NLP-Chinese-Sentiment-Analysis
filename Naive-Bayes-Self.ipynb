{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import random\n",
    "total_positive_line = 500\n",
    "total_negative_line = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ls = []\n",
    "def getStopWord():\n",
    "    with open('lib/stopwords_utf8.txt', 'r',encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            stopword_ls.append(line.split('\\n')[0])\n",
    "getStopWord()\n",
    "\n",
    "def isStopWord(word):\n",
    "    for i in range(len(stopword_ls)):\n",
    "        if word == stopword_ls[i]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def load_data():   #获取数据集\n",
    "    data = []\n",
    "    getStopWord()\n",
    "    positive_line = 0\n",
    "    negative_line = 0\n",
    "\n",
    "    with open('data/total/pos.txt', 'r', encoding='utf-8') as f:\n",
    "        print('File Directory: data/total/pos.txt')\n",
    "        sentences = f.readlines()\n",
    "        for sentence in sentences[:total_positive_line]:\n",
    "            positive_line += 1\n",
    "            if positive_line == total_positive_line:\n",
    "                end_val = '\\n'\n",
    "            else:\n",
    "                end_val = '\\r'\n",
    "            print('Getting positive sentence {}/{}'.format(positive_line,total_positive_line),end=end_val)\n",
    "            word_ls = []\n",
    "            words = sentence.replace('\\n','').split('    ')   #get chinese sentence\n",
    "            tmp_ls = list(jieba.cut(words[1], cut_all=True))   #segmentation\n",
    "            for i in range(len(tmp_ls)):\n",
    "                if not isStopWord(tmp_ls[i]):\n",
    "                       word_ls.append(tmp_ls[i]) \n",
    "            data.append([word_ls, 1])\n",
    "\n",
    "    with open('data/total/neg.txt', 'r', encoding='utf-8') as f:\n",
    "        print('File Directory: data/total/neg.txt')\n",
    "        sentences = f.readlines()\n",
    "        for sentence in sentences[:total_negative_line]:\n",
    "            negative_line += 1\n",
    "            if negative_line == total_negative_line:\n",
    "                end_val = '\\n'\n",
    "            else:\n",
    "                end_val = '\\r'\n",
    "            print('Getting negative sentence {}/{}'.format(negative_line,total_negative_line),end=end_val)\n",
    "            word_ls = []\n",
    "            words = sentence.replace('\\n','').split('    ')   #get chinese sentence\n",
    "            tmp_ls = list(jieba.cut(words[1], cut_all=True))   #segmentation\n",
    "            for i in range(len(tmp_ls)):\n",
    "                if not isStopWord(tmp_ls[i]):\n",
    "                       word_ls.append(tmp_ls[i]) \n",
    "            data.append([word_ls, 0])\n",
    "\n",
    "    print('Positive Line: {} | Negative Line: {}'.format(positive_line,negative_line))\n",
    "    random.shuffle(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Directory: data/total/pos.txt\n",
      "Getting positive sentence 1/500\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.726 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting positive sentence 500/500\n",
      "File Directory: data/total/neg.txt\n",
      "Getting negative sentence 500/500\n",
      "Positive Line: 500 | Negative Line: 500\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "test_size = 1-train_size\n",
    "data_train, data_test = train_test_split(load_data(), test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET | Total Words:37319 | Total Vocabulary:7573 | Max Sentence Length:630\n",
      "TEST  DATASET | Total Words:8996  | Total Vocabulary:3300  | Max Sentence Length:273\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens,_ in data_train for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens,_ in data_train]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"TRAIN DATASET | Total Words:{} | Total Vocabulary:{} | Max Sentence Length:{}\".format(len(all_training_words),len(TRAINING_VOCAB),max(training_sentence_lengths)))\n",
    "\n",
    "all_test_words = [word for tokens,_ in data_test for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens,_ in data_test]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"TEST  DATASET | Total Words:{}  | Total Vocabulary:{}  | Max Sentence Length:{}\" .format(len(all_test_words),len(TEST_VOCAB),max(test_sentence_lengths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_dict(data_train):   #Setting up Vocabulary Dict\n",
    "      print('Setting up Vocabulary Dict...')\n",
    "      vocab = collections.Counter()\n",
    "      for twit,label in data_train:\n",
    "            for word in twit:\n",
    "                  vocab[word] += 1  \n",
    "      print(len(vocab),'Vocabularies')\n",
    "      vocab_ls = list(vocab)\n",
    "      return vocab, vocab_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(data_train=None,data_test=None,vocab_ls=None,test_ls=None):\n",
    "      vocab_len = len(vocab_ls)\n",
    "\n",
    "      #Test individual sentence Word2vec\n",
    "      if not test_ls == None:\n",
    "            word2vec_st = np.zeros([1,vocab_len])\n",
    "            for i in range(len(test_ls)):\n",
    "                  for j in range(vocab_len):\n",
    "                        if test_ls[i] == vocab_ls[j]:\n",
    "                              word2vec_st[0][j] = 1\n",
    "            return word2vec_st\n",
    "      \n",
    "      #Train Dataset Word2vec\n",
    "      length = len(data_train)\n",
    "      word2vec_train = np.zeros([length,vocab_len])\n",
    "      k = 0\n",
    "      for twit,label in data_train:\n",
    "            k += 1\n",
    "            if k == length:\n",
    "                  end_val = ' | '\n",
    "            else:\n",
    "                  end_val = '\\r'\n",
    "            print('Getting Train Dataset Word2vec: {}/{}'.format(k,length),end=end_val)\n",
    "            for i in range(len(twit)):\n",
    "                  for j in range(vocab_len):\n",
    "                        if twit[i] == vocab_ls[j]:\n",
    "                              #word2vec_train[k-1][j] = 1*vocab[vocab_ls[j]]/vocab_len\n",
    "                              word2vec_train[k-1][j] = 1\n",
    "      print('Vector Shape:',word2vec_train.shape)\n",
    "\n",
    "      #Test Dataset Word2vec\n",
    "      length = len(data_test)\n",
    "      word2vec_test = np.zeros([length,vocab_len])\n",
    "      k = 0\n",
    "      for twit,label in data_test:\n",
    "            k += 1\n",
    "            if k == length:\n",
    "                  end_val = ' | '\n",
    "            else:\n",
    "                  end_val = '\\r'\n",
    "            print('Getting Test Dataset Word2vec: {}/{}'.format(k,length),end=end_val)\n",
    "            for i in range(len(twit)):\n",
    "                  for j in range(vocab_len):\n",
    "                        if twit[i] == vocab_ls[j]:\n",
    "                              #word2vec_test[k-1][j] = 1*vocab[vocab_ls[j]]/vocab_len\n",
    "                              word2vec_test[k-1][j] = 1\n",
    "      print('Vector Shape:',word2vec_test.shape)\n",
    "\n",
    "      label_train = [label for tokens,label in data_train]\n",
    "      label_test = [label for tokens,label in data_test]\n",
    "      X_train, X_test = word2vec_train, word2vec_test\n",
    "      y_train, y_test = np.array(label_train), np.array(label_test)\n",
    "      return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Vocabulary Dict...\n",
      "7573 Vocabularies\n",
      "Getting Train Dataset Word2vec: 800/800 | Vector Shape: (800, 7573)\n",
      "Getting Test Dataset Word2vec: 200/200 | Vector Shape: (200, 7573)\n"
     ]
    }
   ],
   "source": [
    "vocab, vocab_ls = vocab_dict(data_train)\n",
    "X_train, X_test, y_train, y_test = word2vec(data_train=data_train, data_test=data_test, vocab_ls=vocab_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pos,dic_neg,dic_all = {},{},{}\n",
    "for i in range(len(X_train)):\n",
    "            if y_train[i] == 1:\n",
    "                  if tuple(X_train[i]) in dic_pos:\n",
    "                        dic_pos[tuple(X_train[i])] += 1\n",
    "                  else:\n",
    "                        dic_pos[tuple(X_train[i])] = 1\n",
    "            else:\n",
    "                  if tuple(X_train[i]) in dic_neg:\n",
    "                        dic_neg[tuple(X_train[i])] += 1\n",
    "                  else:\n",
    "                        dic_neg[tuple(X_train[i])] = 1\n",
    "            if tuple(X_train[i]) in dic_all:\n",
    "                  dic_all[tuple(X_train[i])] += 1\n",
    "            else:\n",
    "                  dic_all[tuple(X_train[i])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_pos_text,dic_neg_text={},{}\n",
    "for i in range(len(data_train)):\n",
    "      if data_train[i][1] == 1:\n",
    "            for j in range(len(data_train[i][0])):      \n",
    "                  if data_train[i][0][j] in dic_pos_text:\n",
    "                        dic_pos_text[data_train[i][0][j]] += 1\n",
    "                  else:\n",
    "                        dic_pos_text[data_train[i][0][j]] = 1\n",
    "      else:\n",
    "            for j in range(len(data_train[i][0])):  \n",
    "                  if data_train[i][0][j] in dic_neg_text:\n",
    "                        dic_neg_text[data_train[i][0][j]] += 1\n",
    "                  else:\n",
    "                        dic_neg_text[data_train[i][0][j]]= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_segment(text):\n",
    "    word_ls = []\n",
    "    tmp_ls = list(jieba.cut(text, cut_all=False))   #segmentation\n",
    "    for i in range(len(tmp_ls)):\n",
    "        if not isStopWord(tmp_ls[i]):\n",
    "            word_ls.append(tmp_ls[i]) \n",
    "    return word_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes2\n",
    "\n",
    "def Bayes2(X_data=None,y_data=None,test_word2vec=None):\n",
    "      if not test_word2vec == None:\n",
    "            train_positive_line = list(test_word2vec).count(1)\n",
    "            train_negative_line = list(test_word2vec).count(0)\n",
    "            prob_c_pos = train_positive_line/(train_negative_line+train_positive_line)\n",
    "            prob_c_neg = train_negative_line/(train_negative_line+train_positive_line)\n",
    "            prob_x_c_pos,prob_x_c_neg = 0,0\n",
    "            x = tuple(test_word2vec)\n",
    "            if x in dic_pos:\n",
    "                  prob_x_c_pos = dic_pos[x] / train_positive_line\n",
    "            if x in dic_neg:\n",
    "                  prob_x_c_neg = dic_neg[x] / train_negative_line\n",
    "            if x in dic_all:\n",
    "                  prob_x = dic_all[x] / (train_negative_line+train_positive_line)\n",
    "            else:\n",
    "                  prob_x = 1\n",
    "            prob_c_x_pos = prob_x_c_pos * prob_c_pos / prob_x\n",
    "            prob_c_x_neg = prob_x_c_neg * prob_c_neg / prob_x\n",
    "            if prob_c_x_pos <= prob_c_x_neg: \n",
    "                  guess_label = 1\n",
    "            else:\n",
    "                  guess_label = 0\n",
    "            return guess_label\n",
    "            \n",
    "      guess_y = []\n",
    "      train_positive_line = list(y_data).count(1)\n",
    "      train_negative_line = list(y_data).count(0)\n",
    "      prob_c_pos = train_positive_line/(train_negative_line+train_positive_line)\n",
    "      prob_c_neg = train_negative_line/(train_negative_line+train_positive_line)\n",
    "      for i in range(len(X_data)):\n",
    "            prob_x_c_pos,prob_x_c_neg = 0,0\n",
    "            x = tuple(X_data[i])\n",
    "            if x in dic_pos:\n",
    "                  prob_x_c_pos = dic_pos[x] / train_positive_line\n",
    "            if x in dic_neg:\n",
    "                  prob_x_c_neg = dic_neg[x] / train_negative_line\n",
    "            if x in dic_all:\n",
    "                  prob_x = dic_all[x] / (train_negative_line+train_positive_line)\n",
    "            else:\n",
    "                  prob_x = 1\n",
    "            prob_c_x_pos = prob_x_c_pos * prob_c_pos / prob_x\n",
    "            prob_c_x_neg = prob_x_c_neg * prob_c_neg / prob_x\n",
    "            if prob_c_x_pos >= prob_c_x_neg: \n",
    "                  guess_label = 1\n",
    "            else:\n",
    "                  guess_label = 0\n",
    "            guess_y.append(guess_label)\n",
    "\n",
    "      correct_count = 0\n",
    "      for i in range(len(y_data)):\n",
    "            if y_data[i] == guess_y[i]:\n",
    "                  correct_count += 1\n",
    "      print('Accuracy: {}'.format(correct_count/(train_negative_line+train_positive_line)))\n",
    "      return \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes\n",
    "\n",
    "def Bayes(X_data=None,y_data=None,test_word2vec=None,output=True):\n",
    "      if not test_word2vec == None:\n",
    "            train_positive_line = list(test_word2vec).count(1)\n",
    "            train_negative_line = list(test_word2vec).count(0)\n",
    "            prob_c_pos = train_positive_line/(train_negative_line+train_positive_line)\n",
    "            prob_c_neg = train_negative_line/(train_negative_line+train_positive_line)\n",
    "            prob_xi_c_pos,prob_xi_c_neg = 1,1\n",
    "            for j in range(len(test_word2vec)):\n",
    "                  if(test_word2vec[j] == 1):\n",
    "                        real_word = vocab_ls[j]\n",
    "                        if real_word in dic_pos_text:\n",
    "                              if not train_positive_line == 0:\n",
    "                                    prob_xi_c_pos *= (dic_pos_text[real_word] / train_positive_line * prob_c_pos)\n",
    "                        if real_word in dic_neg_text:\n",
    "                              if not train_negative_line == 0:\n",
    "                                    prob_xi_c_neg *= (dic_neg_text[real_word] / train_negative_line *prob_c_neg)\n",
    "            prob_c_x_pos = prob_xi_c_pos * prob_c_pos \n",
    "            prob_c_x_neg = prob_xi_c_neg * prob_c_neg \n",
    "            if not prob_c_x_pos+prob_c_x_neg == 0:\n",
    "                  prob_pos = (1 - prob_c_x_pos/(prob_c_x_pos+prob_c_x_neg)) * 100\n",
    "                  prob_neg = (1 - prob_c_x_neg/(prob_c_x_pos+prob_c_x_neg)) * 100\n",
    "            if prob_c_x_pos <= prob_c_x_neg: \n",
    "                  guess_label = 1\n",
    "            else:\n",
    "                  guess_label = 0\n",
    "            if output:\n",
    "                  print('Positive Probability = {:.4f}%'.format(prob_pos))\n",
    "                  print('Negative Probability = {:.4f}%'.format(prob_neg))\n",
    "            return guess_label\n",
    "            \n",
    "      guess_y = []\n",
    "      train_positive_line = list(y_data).count(1)\n",
    "      train_negative_line = list(y_data).count(0)\n",
    "      prob_c_pos = train_positive_line/(train_negative_line+train_positive_line)\n",
    "      prob_c_neg = train_negative_line/(train_negative_line+train_positive_line)\n",
    "      for vector in X_data:\n",
    "            prob_xi_c_pos,prob_xi_c_neg = 1,1\n",
    "            for j in range(len(vector)):\n",
    "                  if(vector[j] == 1):\n",
    "                        real_word = vocab_ls[j]\n",
    "                        if real_word in dic_pos_text:\n",
    "                              if not train_positive_line == 0:\n",
    "                                    prob_xi_c_pos *= (dic_pos_text[real_word] / train_positive_line * prob_c_pos)\n",
    "                        if real_word in dic_neg_text:\n",
    "                              if not train_negative_line == 0:\n",
    "                                    prob_xi_c_neg *= (dic_neg_text[real_word] / train_negative_line *prob_c_neg)\n",
    "            prob_c_x_pos = prob_xi_c_pos * prob_c_pos\n",
    "            prob_c_x_neg = prob_xi_c_neg * prob_c_neg \n",
    "\n",
    "            if prob_c_x_pos <= prob_c_x_neg: \n",
    "                  guess_label = 1\n",
    "            else:\n",
    "                  guess_label = 0\n",
    "            guess_y.append(guess_label)\n",
    "\n",
    "      correct_count = 0\n",
    "      for i in range(len(y_data)):\n",
    "            if y_data[i] == guess_y[i]:\n",
    "                  correct_count += 1\n",
    "      print('Accuracy: {}'.format(correct_count/(train_negative_line+train_positive_line)))\n",
    "      return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91375\n"
     ]
    }
   ],
   "source": [
    "Bayes(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.695\n"
     ]
    }
   ],
   "source": [
    "Bayes(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample 1\n",
      "房间设施难以够得上五星级，服务还不错，有送水果。\n",
      "Positive Probability = 98.8341%\n",
      "Negative Probability = 1.1659%\n",
      "Positive Sentiment 正面情绪\n",
      "\n",
      "Test Sample 2\n",
      "前台服务较差，不为客户着想。房间有朋友来需要打扫，呼叫了两个小时也未打扫。房间下水道臭气熏天，卫生间漏水堵水。\n",
      "Positive Probability = 0.1276%\n",
      "Negative Probability = 99.8724%\n",
      "Negative Sentiment 负面情绪\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"房间设施难以够得上五星级，服务还不错，有送水果。\"\n",
    "print('Test Sample 1\\n'+text)\n",
    "tokens = jieba_segment(text)\n",
    "word2vec_st = word2vec(None,None,vocab_ls=vocab_ls,test_ls=tokens)\n",
    "sentiment = Bayes(None,None,list(word2vec_st[0]))\n",
    "if sentiment == 1:\n",
    "      print(\"Positive Sentiment 正面情绪\\n\")\n",
    "else:\n",
    "      print(\"Negative Sentiment 负面情绪\\n\")\n",
    "\n",
    "text = \"前台服务较差，不为客户着想。房间有朋友来需要打扫，呼叫了两个小时也未打扫。房间下水道臭气熏天，卫生间漏水堵水。\"\n",
    "print('Test Sample 2\\n'+text)\n",
    "tokens = jieba_segment(text)\n",
    "word2vec_st = word2vec(None,None,vocab_ls=vocab_ls,test_ls=tokens)\n",
    "sentiment = Bayes(None,None,list(word2vec_st[0]))\n",
    "if sentiment == 1:\n",
    "      print(\"Positive Sentiment 正面情绪\\n\")\n",
    "else:\n",
    "      print(\"Negative Sentiment 负面情绪\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Naive Bayes Test Data----------\n",
      "Getting positive data: 100/100\n",
      "Positive Count: 78 | Negative Count: 22\n",
      "Getting negative data: 100/100\n",
      "Positive Count: 52 | Negative Count: 48\n",
      "Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "#TEST DATASET\n",
    "print('---------Naive Bayes Test Data----------')\n",
    "dataset_size = 100\n",
    "correct_count = 0\n",
    "type = ['pos','neg']\n",
    "for j in range(len(type)):\n",
    "      positive_count,negative_count = 0,0\n",
    "      if type[j] == 'pos':\n",
    "            st = 'positive'\n",
    "      else:\n",
    "            st = 'negative'\n",
    "      for i in range(dataset_size):\n",
    "            with open('data/'+st+'/'+type[j]+'.'+str(i)+'.txt','r',encoding='UTF-8') as file:\n",
    "                  text = file.read().replace('\\n', '')\n",
    "                  if i == dataset_size-1:\n",
    "                        end_val = '\\n'\n",
    "                  else:\n",
    "                        end_val = '\\r'\n",
    "                  \n",
    "                  print('Getting '+st+' data: {}/{}'.format(i+1,dataset_size),end=end_val)\n",
    "\n",
    "            tokens = jieba_segment(text)\n",
    "            word2vec_st = word2vec(None,None,vocab_ls=vocab_ls,test_ls=tokens)\n",
    "            sentiment = Bayes(None,None,list(word2vec_st[0]),output=False)\n",
    "            if sentiment == 1:\n",
    "                  #print(\"Positive Sentiment\")\n",
    "                  positive_count += 1\n",
    "            else:\n",
    "                  #print(\"Negative Sentiment\")\n",
    "                  negative_count += 1\n",
    "\n",
    "      print('Positive Count:',positive_count,end = ' | ')\n",
    "      print('Negative Count:',negative_count)\n",
    "      if type[j] == 'pos':\n",
    "            correct_count += positive_count\n",
    "      else:\n",
    "            correct_count += negative_count\n",
    "print('Accuracy: {}'.format(correct_count/(2*dataset_size)))\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db9c657620c8578ddfc3d9ba89c38fdc5fd80f960e156f76bd6d1a512183127c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
